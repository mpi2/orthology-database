image: docker:stable

variables:
   # When using dind service we need to instruct docker, to talk with the
   # daemon started inside of the service. The daemon is available with
   # a network connection instead of the default /var/run/docker.sock socket.
   #
   # The 'docker' hostname is the alias of the service container as described at
   # https://docs.gitlab.com/ee/ci/docker/using_docker_images.html#accessing-the-services
   #
   # Note that if you're using the Kubernetes executor, the variable should be set to
   # tcp://localhost:2375/ because of how the Kubernetes executor connects services
   # to the job container
   # DOCKER_HOST: tcp://localhost:2375/
   #
   # For non-Kubernetes executors, we use tcp://docker:2375/
   DOCKER_HOST: tcp://docker:2375/
   # When using dind, it's wise to use the overlayfs driver for
   # improved performance.
   DOCKER_DRIVER: overlay2
   
   # Since the docker:dind container and the runner container don’t share their root
   # filesystem, the job’s working directory can be used as a mount point for children
   # containers. For example, if you have files you want to share with a child container,
   # you may create a subdirectory under /builds/$CI_PROJECT_PATH and use it as your
   # mount point.
   MOUNT_POINT: /builds/$CI_PROJECT_PATH/mnt
   
   # For EBI you need to override the definition of CI_REGISTRY to remove the port number
   CI_REGISTRY: dockerhub.ebi.ac.uk
   CI_REGISTRY_IMAGE: $CI_REGISTRY/$CI_PROJECT_PATH

   #NOW: $(date '+%Y-%m-%d-%H-%M-%S')
   #NOW: $(date '+%Y-%m-%d')
   
   # To solve the issue with the Docker in Docker 19.03 service.
   # Logged as: GitLab.com CI jobs failing if using docker:stable-dind image
   # see: https://gitlab.com/gitlab-com/gl-infra/production/issues/982
   DOCKER_TLS_CERTDIR: ""

services:
   - docker:dind

# Use this command to look at your docker environment
# Note: This step can be overwritten by before_script sections in specific jobs.
#
#before_script:
#   - docker info


stages:
   - download
   - synonym-formatting
   - mgi-formatting
   - build
   - dev-deploy
   - dev-test
   - production-deploy
   - production-test


#
# Data download stage
#



HGNC_data:
    stage: download
    before_script:
        - apk add --update curl && rm -rf /var/cache/apk/*
    script:
        
        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"
        
        # Fetch data files for the service
        # HGNC_data
        - curl -sSLN -O ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/alternative_loci_set.txt
        - curl -sSLN -O ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/non_alt_loci_set.txt
        - curl -sSLN -O ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt

         # HCOP_data
        - curl -sSLN -O ftp://ftp.ebi.ac.uk/pub/databases/genenames/hcop/human_mouse_hcop_fifteen_column.txt.gz && gunzip human_mouse_hcop_fifteen_column.txt.gz

    artifacts:
        paths:
            - "$MOUNT_POINT/"



MGI_data:
    stage: download
    before_script:
        - apk add --update curl && rm -rf /var/cache/apk/*
    script:

        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"
        
        # MGI Gene data
        - curl -sSLN -O http://www.informatics.jax.org/downloads/reports/MGI_Gene_Model_Coord.rpt
        # Remove the additional tab at the end of the file
        - sed 's/[[:space:]]*$//' MGI_Gene_Model_Coord.rpt > MGI_Gene_Model_Coord.rpt.tmp && mv MGI_Gene_Model_Coord.rpt.tmp MGI_Gene_Model_Coord.rpt

        # MGI_Mrk_List2_data
        - curl -sSLN -O http://www.informatics.jax.org/downloads/reports/MRK_List2.rpt

    artifacts:
        paths:
             - "$MOUNT_POINT/"

#
# Data formatting stage
#


Synonym-formatting:
    stage: synonym-formatting
    before_script:
        - apk add --update python3 && rm -rf /var/cache/apk/*
    script:
        # Extract the Human gene synonyms - this creates the file HGNC_synonyms.txt
        - python3 "$CI_PROJECT_DIR"/scripts/HGNCPreProcessor.py "$MOUNT_POINT"/alternative_loci_set.txt "$MOUNT_POINT"/non_alt_loci_set.txt

        # Extract the mouse gene synonyms - this creates the file Mrk_synonyms.txt
        - python3 "$CI_PROJECT_DIR"/scripts/MgiMrkPreProcessor.py "$MOUNT_POINT"/MRK_List2.rpt

    dependencies:
        - HGNC_data
        - MGI_data
    artifacts:
        paths:
            - "$MOUNT_POINT/"


build_image:
    stage: build
    before_script:
        - echo "${CI_REGISTRY_PASSWORD}" | docker login -u "${CI_REGISTRY_USER}" --password-stdin  ${CI_REGISTRY}
        - NOW=$(date '+%Y-%m-%d-%H-%M-%S')
        - echo "export NOW=${NOW}" > ${MOUNT_POINT}/datetime.env
    script:

        - export
        - source ${MOUNT_POINT}/datetime.env
        - echo ${NOW}
        - docker build -t orthology-db .  | tee ${MOUNT_POINT}/build.log
        - docker run --name orthodbcontainer -v "$MOUNT_POINT:/mnt" -d orthology-db

        # Time is required to load the data
        # Hence the long pause period. 
        #
        # An alternative would be to copy the data required into to the image
        # when it is built, but this is not trivial in this build system, and a downside  would be
        # that every time the container starts there would always be a lag while the data was loaded.
        
        - sleep "${DB_LOAD_TIME}"

        - docker exec orthodbcontainer sh /usr/local/data/postgres_processing_time.sh
        - docker exec orthodbcontainer sh -c "rm -r /usr/local/data"
        
        - echo "${CI_REGISTRY_IMAGE}":"${NOW}"
        - docker commit orthodbcontainer "${CI_REGISTRY_IMAGE}":"${NOW}"
        
        - docker tag "${CI_REGISTRY_IMAGE}":"${NOW}" "${CI_REGISTRY_IMAGE}":latest
        - docker push "${CI_REGISTRY_IMAGE}"  | tee ${MOUNT_POINT}/push.log

        - docker logout ${CI_REGISTRY}

        # PUSH THE IMAGE TO DOCKERHUB
        - echo "${DOCKER_HUB_PWD}" | docker login -u "${DOCKER_HUB_USER}" --password-stdin
 
        - docker tag "${CI_REGISTRY_IMAGE}":"${NOW}" "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":"${NOW}"
        - docker tag "${CI_REGISTRY_IMAGE}":"${NOW}" "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":latest
        - docker push "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}"  | tee ${MOUNT_POINT}/dockerhub-push-latest.log

        - docker logout
        


    dependencies:
        - Synonym-formatting
    artifacts:
        name: "database-${CI_JOB_ID}"
        paths:
            - ${MOUNT_POINT}


#dev-deploy:
#  stage: dev-deploy
#  image: dtzar/helm-kubectl:2.13.0
#  only:
#    refs:
#      - master
#  script:
#    - source ${MOUNT_POINT}/datetime.env
#    - echo ${NOW}
#    #
#    - kubectl config set-cluster local --server="${KUBERNETES_ENDPOINT}"
#    - kubectl config set clusters.local.certificate-authority-data "${KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
#    - kubectl config set-credentials "${KUBERNETES_DEV_USER}" --token="${KUBERNETES_DEV_USER_TOKEN}"
#    - kubectl config set-context "${KUBERNETES_DEV_NAMESPACE}" --cluster=local --user=${KUBERNETES_DEV_USER} --namespace="${KUBERNETES_DEV_NAMESPACE}"
#    - kubectl config use-context "${KUBERNETES_DEV_NAMESPACE}"
#    - kubectl version
#    #
#    - sed -i "s/latest/${NOW}/g" kube/dev/database/database-deployment.yaml
#    - sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/dev/database/database-deployment.yaml
#    
#    - |
#      if kubectl apply -f kube/dev/database/database-deployment.yaml --record | grep -q unchanged; then
#          echo "=> Patching deployment to force image update."
#          kubectl patch -f kube/dev/database/database-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
#      else
#          echo "=> Deployment apply has changed the object, no need to force image update."
#      fi
#
#
#    - kubectl rollout status -f kube/dev/database/database-deployment.yaml
#    - kubectl get all,ing 



hh-dev-deploy:
  stage: dev-deploy
  # Use an image with helm v2.14.3, kubectl v1.15.2, alpine 3.10
  # rancher/hyperkube:v1.15.3-rancher1 installed on the hh cluster
  image: dtzar/helm-kubectl:2.14.3
  only:
    refs:
      - master
  script:
    - source ${MOUNT_POINT}/datetime.env
    - echo ${NOW}
    #
    - kubectl config set-cluster local --server="${HH_KUBERNETES_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${HH_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
    - kubectl config set-credentials ${HH_KUBERNETES_DEV_USER} --token="${HH_KUBERNETES_DEV_USER_TOKEN}"
    - kubectl config set-context "${HH_KUBERNETES_DEV_NAMESPACE}" --cluster=local --user=${HH_KUBERNETES_DEV_USER} --namespace="${HH_KUBERNETES_DEV_NAMESPACE}"
    - kubectl config use-context "${HH_KUBERNETES_DEV_NAMESPACE}"
    - kubectl version
    #
    - sed -i "s/latest/${NOW}/g" kube/wp/dev/database/database-deployment.yaml
    - sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/dev/database/database-deployment.yaml
    
    - |
      if kubectl apply -f kube/wp/dev/database/database-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/dev/database/database-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi


    - kubectl rollout status -f kube/wp/dev/database/database-deployment.yaml
    - kubectl get pod,deployment,rs



hx-dev-deploy:
  stage: dev-deploy
  # Use an image with helm v2.14.3, kubectl v1.15.2, alpine 3.10
  # rancher/hyperkube:v1.15.3-rancher1 installed on the hh cluster
  image: dtzar/helm-kubectl:2.14.3
  only:
    refs:
      - master
  script:
    - source ${MOUNT_POINT}/datetime.env
    - echo ${NOW}
    #
    - kubectl config set-cluster local --server="${HX_KUBERNETES_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${HX_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
    - kubectl config set-credentials ${HX_KUBERNETES_DEV_USER} --token="${HX_KUBERNETES_DEV_USER_TOKEN}"
    - kubectl config set-context "${HX_KUBERNETES_DEV_NAMESPACE}" --cluster=local --user=${HX_KUBERNETES_DEV_USER} --namespace="${HX_KUBERNETES_DEV_NAMESPACE}"
    - kubectl config use-context "${HX_KUBERNETES_DEV_NAMESPACE}"
    - kubectl version
    #
    - sed -i "s/latest/${NOW}/g" kube/wp/dev/database/database-deployment.yaml
    - sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/dev/database/database-deployment.yaml
    
    - |
      if kubectl apply -f kube/wp/dev/database/database-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/dev/database/database-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi


    - kubectl rollout status -f kube/wp/dev/database/database-deployment.yaml
    - kubectl get pod,deployment,rs




hh-dev-test:
    image: ubuntu:latest
    stage: dev-test
    before_script:
        - apt-get update && apt-get install -y curl && apt-get clean && rm -rf /var/lib/apt/lists/*
    script:
        
        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"
        
        - source "$CI_PROJECT_DIR"/scripts/service_test.sh -s="${HH_KUBERNETES_INTERNAL_ENDPOINT}/${HH_KUBERNETES_DEV_NAMESPACE}/v1/graphql"  | tee ${MOUNT_POINT}/hh-dev-test.log
        

    artifacts:
        paths:
            - "$MOUNT_POINT/"




hx-dev-test:
    image: ubuntu:latest
    stage: dev-test
    before_script:
        - apt-get update && apt-get install -y curl && apt-get clean && rm -rf /var/lib/apt/lists/*
    script:
        
        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"
        
        - source "$CI_PROJECT_DIR"/scripts/service_test.sh -s="${HX_KUBERNETES_INTERNAL_ENDPOINT}/${HX_KUBERNETES_DEV_NAMESPACE}/v1/graphql"  | tee ${MOUNT_POINT}/hx-dev-test.log
        

    artifacts:
        paths:
            - "$MOUNT_POINT/"



#production-deploy:
#  stage: production-deploy
#  image: dtzar/helm-kubectl:2.13.0
#  only:
#    refs:
#      - master
#  script:
#    - source ${MOUNT_POINT}/datetime.env
#    - echo ${NOW}
#    #
#    - kubectl config set-cluster local --server="${KUBERNETES_ENDPOINT}"
#    - kubectl config set clusters.local.certificate-authority-data "${KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
#    - kubectl config set-credentials "${KUBERNETES_USER}" --token="${KUBERNETES_USER_TOKEN}"
#    - kubectl config set-context "${KUBERNETES_NAMESPACE}" --cluster=local --user=${KUBERNETES_USER} --namespace="${KUBERNETES_NAMESPACE}"
#    - kubectl config use-context "${KUBERNETES_NAMESPACE}"
#    - kubectl version
#    #
#    - sed -i "s/latest/${NOW}/g" kube/production/database/database-deployment.yaml
#    - sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/production/database/database-deployment.yaml
#    
#    - |
#      if kubectl apply -f kube/production/database/database-deployment.yaml --record | grep -q unchanged; then
#          echo "=> Patching deployment to force image update."
#          kubectl patch -f kube/production/database/database-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
#      else
#          echo "=> Deployment apply has changed the object, no need to force image update."
#      fi
#
#
#    - kubectl rollout status -f kube/production/database/database-deployment.yaml
#    - kubectl get all,ing 




hh-production-deploy:
  stage: production-deploy
  # Use an image with helm v2.14.3, kubectl v1.15.2, alpine 3.10
  # rancher/hyperkube:v1.15.3-rancher1 installed on the hh cluster
  image: dtzar/helm-kubectl:2.14.3
  only:
    refs:
      - master
  script:
    - source ${MOUNT_POINT}/datetime.env
    - echo ${NOW}
    #
    - kubectl config set-cluster local --server="${HH_KUBERNETES_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${HH_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
    - kubectl config set-credentials ${HH_KUBERNETES_USER} --token="${HH_KUBERNETES_USER_TOKEN}"
    - kubectl config set-context "${HH_KUBERNETES_NAMESPACE}" --cluster=local --user=${HH_KUBERNETES_USER} --namespace="${HH_KUBERNETES_NAMESPACE}"
    - kubectl config use-context "${HH_KUBERNETES_NAMESPACE}"
    - kubectl version
    #
    - sed -i "s/latest/${NOW}/g" kube/wp/production/database/database-deployment.yaml
    - sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/production/database/database-deployment.yaml
    
    - |
      if kubectl apply -f kube/wp/production/database/database-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/production/database/database-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi


    - kubectl rollout status -f kube/wp/production/database/database-deployment.yaml
    - kubectl get pod,deployment,rs



hx-production-deploy:
  stage: production-deploy
  # Use an image with helm v2.14.3, kubectl v1.15.2, alpine 3.10
  # rancher/hyperkube:v1.15.3-rancher1 installed on the hh cluster
  image: dtzar/helm-kubectl:2.14.3
  only:
    refs:
      - master
  script:
    - source ${MOUNT_POINT}/datetime.env
    - echo ${NOW}
    #
    - kubectl config set-cluster local --server="${HX_KUBERNETES_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${HX_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
    - kubectl config set-credentials ${HX_KUBERNETES_USER} --token="${HX_KUBERNETES_USER_TOKEN}"
    - kubectl config set-context "${HX_KUBERNETES_NAMESPACE}" --cluster=local --user=${HX_KUBERNETES_USER} --namespace="${HX_KUBERNETES_NAMESPACE}"
    - kubectl config use-context "${HX_KUBERNETES_NAMESPACE}"
    - kubectl version
    #
    - sed -i "s/latest/${NOW}/g" kube/wp/production/database/database-deployment.yaml
    - sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/production/database/database-deployment.yaml
    
    - |
      if kubectl apply -f kube/wp/production/database/database-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/production/database/database-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi


    - kubectl rollout status -f kube/wp/production/database/database-deployment.yaml
    - kubectl get pod,deployment,rs





hh-production-test:
    image: ubuntu:latest
    stage: production-test
    before_script:
        - apt-get update && apt-get install -y curl && apt-get clean && rm -rf /var/lib/apt/lists/*
    script:
        
        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"
        
        - source "$CI_PROJECT_DIR"/scripts/service_test.sh -s="${HH_KUBERNETES_INTERNAL_ENDPOINT}/${HH_KUBERNETES_NAMESPACE}/v1/graphql"  | tee ${MOUNT_POINT}/hh-production-test.log
        

    artifacts:
        paths:
            - "$MOUNT_POINT/"




hx-production-test:
    image: ubuntu:latest
    stage: production-test
    before_script:
        - apt-get update && apt-get install -y curl && apt-get clean && rm -rf /var/lib/apt/lists/*
    script:
        
        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"
        
        - source "$CI_PROJECT_DIR"/scripts/service_test.sh -s="${HX_KUBERNETES_INTERNAL_ENDPOINT}/${HX_KUBERNETES_NAMESPACE}/v1/graphql"  | tee ${MOUNT_POINT}/hx-production-test.log
        

    artifacts:
        paths:
            - "$MOUNT_POINT/"

