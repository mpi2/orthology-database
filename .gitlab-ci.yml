image: $CI_REGISTRY/mouse-informatics/docker:latest

variables:
   # When using dind service we need to instruct docker, to talk with the
   # daemon started inside of the service. The daemon is available with
   # a network connection instead of the default /var/run/docker.sock socket.
   #
   # The 'docker' hostname is the alias of the service container as described at
   # https://docs.gitlab.com/ee/ci/docker/using_docker_images.html#accessing-the-services
   #
   # Note that if you're using the Kubernetes executor, the variable should be set to
   # tcp://localhost:2375/ because of how the Kubernetes executor connects services
   # to the job container
   # DOCKER_HOST: tcp://localhost:2375/
   #
   # For non-Kubernetes executors, we use tcp://docker:2375/
   DOCKER_HOST: tcp://docker:2375/
   # When using dind, it's wise to use the overlayfs driver for
   # improved performance.
   DOCKER_DRIVER: overlay2
   
   # Since the docker:dind container and the runner container don’t share their root
   # filesystem, the job’s working directory can be used as a mount point for children
   # containers. For example, if you have files you want to share with a child container,
   # you may create a subdirectory under /builds/$CI_PROJECT_PATH and use it as your
   # mount point.
   MOUNT_POINT: /builds/$CI_PROJECT_PATH/mnt
   
   # For EBI you need to override the definition of CI_REGISTRY to remove the port number
   CI_REGISTRY: dockerhub.ebi.ac.uk
   CI_REGISTRY_IMAGE: $CI_REGISTRY/$CI_PROJECT_PATH

   #NOW: $(date '+%Y-%m-%d-%H-%M-%S')
   #NOW: $(date '+%Y-%m-%d')
   
   # To solve the issue with the Docker in Docker 19.03 service.
   # Logged as: GitLab.com CI jobs failing if using docker:stable-dind image
   # see: https://gitlab.com/gitlab-com/gl-infra/production/issues/982
   DOCKER_TLS_CERTDIR: ""
   SCAN_KUBERNETES_MANIFESTS: "true"


# Use this command to look at your docker environment
# Note: This step can be overwritten by before_script sections in specific jobs.
#
#before_script:
#   - docker info

include:
  - template: Security/SAST.gitlab-ci.yml
  - template: Security/Secret-Detection.gitlab-ci.yml


stages:
   - download
   - synonym-formatting
   - mgi-formatting
   - build
   - test
   - dev-deploy
   - dev-test
   - production-deploy
   - production-test


#
# Data download stage
#



HGNC_data:
    stage: download
    before_script:
        - apk add --update curl && rm -rf /var/cache/apk/*
    script:
        
        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"
        
        # Fetch data files for the service
        # HGNC_data
        - curl -sSLN -O https://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/alternative_loci_set.txt
        - curl -sSLN -O https://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/non_alt_loci_set.txt
        - curl -sSLN -O https://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/hgnc_complete_set.txt

         # HCOP_data
        - curl -sSLN -O https://ftp.ebi.ac.uk/pub/databases/genenames/hcop/human_mouse_hcop_fifteen_column.txt.gz && gunzip human_mouse_hcop_fifteen_column.txt.gz

    artifacts:
        paths:
            - "$MOUNT_POINT/"



MGI_data:
    stage: download
    before_script:
        - apk add --update curl && rm -rf /var/cache/apk/*
    script:

        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"
        
        # MGI Gene data
        - curl -sSLN -O http://www.informatics.jax.org/downloads/reports/MGI_Gene_Model_Coord.rpt
        # Remove the additional tab at the end of the file
        - sed 's/[[:space:]]*$//' MGI_Gene_Model_Coord.rpt > MGI_Gene_Model_Coord.rpt.tmp && mv MGI_Gene_Model_Coord.rpt.tmp MGI_Gene_Model_Coord.rpt

        # MGI_Mrk_List2_data
        - curl -sSLN -O http://www.informatics.jax.org/downloads/reports/MRK_List2.rpt

    artifacts:
        paths:
             - "$MOUNT_POINT/"

#
# Data formatting stage
#


Synonym-formatting:
    stage: synonym-formatting
    before_script:
        - apk add --update python3 && rm -rf /var/cache/apk/*
    script:
        # Extract the Human gene synonyms - this creates the file HGNC_synonyms.txt
        - python3 "$CI_PROJECT_DIR"/scripts/HGNCPreProcessor.py "$MOUNT_POINT"/alternative_loci_set.txt "$MOUNT_POINT"/non_alt_loci_set.txt

        # Extract the mouse gene synonyms - this creates the file Mrk_synonyms.txt
        - python3 "$CI_PROJECT_DIR"/scripts/MgiMrkPreProcessor.py "$MOUNT_POINT"/MRK_List2.rpt

    dependencies:
        - HGNC_data
        - MGI_data
    artifacts:
        paths:
            - "$MOUNT_POINT/"


build_image:
    stage: build
    services:
        - name: $CI_REGISTRY/mouse-informatics/dind:latest
          alias: docker
    before_script:
        - echo "${CI_REGISTRY_PASSWORD}" | docker login -u "${CI_REGISTRY_USER}" --password-stdin  ${CI_REGISTRY}
        - NOW=$(date '+%Y-%m-%d-%H-%M-%S')
        - echo "export NOW=${NOW}" > ${MOUNT_POINT}/datetime.env
    script:

        - export
        - source ${MOUNT_POINT}/datetime.env
        - echo ${NOW}
        - sed -i "s|postgres|${LOCAL_GITLAB_POSTGRES_IMAGE}|g" Dockerfile

        - docker build -t orthology-db .  | tee ${MOUNT_POINT}/build.log
        - docker run --name orthodbcontainer -v "$MOUNT_POINT:/mnt" -d orthology-db

        # Time is required to load the data
        # Hence the long pause period. 
        #
        # An alternative would be to copy the data required into to the image
        # when it is built, but this is not trivial in this build system, and a downside  would be
        # that every time the container starts there would always be a lag while the data was loaded.
        
        - sleep "${DB_LOAD_TIME}"

        - docker exec orthodbcontainer sh /usr/local/data/postgres_processing_time.sh
        - docker cp orthodbcontainer:/usr/local/data/database.sql "$MOUNT_POINT"/database.sql
        - docker exec orthodbcontainer sh -c "rm -r /usr/local/data"
        - docker stop -t 120 orthodbcontainer
        
        - sed -i "s|FROM postgres|FROM ${LOCAL_GITLAB_POSTGRES_IMAGE}|g" Dockerfile-production
        - docker build -t prod-orthology-db -f Dockerfile-production .  | tee ${MOUNT_POINT}/build-production.log
        - docker run --name prodorthodbcontainer -v "$MOUNT_POINT:/mnt" -d prod-orthology-db
        - sleep 120

        - echo "${CI_REGISTRY_IMAGE}":"${NOW}"

        - docker stop -t 120 prodorthodbcontainer
        - docker commit prodorthodbcontainer "${CI_REGISTRY_IMAGE}":"${NOW}"
        
        - docker tag "${CI_REGISTRY_IMAGE}":"${NOW}" "${CI_REGISTRY_IMAGE}":latest
        - docker push "${CI_REGISTRY_IMAGE}":"${NOW}"  | tee ${MOUNT_POINT}/push.log
        - docker push "${CI_REGISTRY_IMAGE}":latest  | tee ${MOUNT_POINT}/push.log

        - docker logout ${CI_REGISTRY}

        # PUSH THE IMAGE TO DOCKERHUB
        - echo "${DOCKER_HUB_PWD}" | docker login -u "${DOCKER_HUB_USER}" --password-stdin
 
        - docker tag "${CI_REGISTRY_IMAGE}":"${NOW}" "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":"${NOW}"
        - docker push "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":"${NOW}"  | tee ${MOUNT_POINT}/dockerhub-push-latest.log

        - docker tag "${CI_REGISTRY_IMAGE}":"${NOW}" "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":latest
        - docker push "${DOCKER_HUB_USER}"/"${DOCKER_HUB_REPO}":latest  | tee ${MOUNT_POINT}/dockerhub-push-latest.log

        - docker logout
        


    dependencies:
        - Synonym-formatting
    artifacts:
        name: "database-${CI_JOB_ID}"
        paths:
            - ${MOUNT_POINT}



sast:
  stage: test
  script:
    - echo "Running SAST scan ..."

  artifacts:
    reports:
      container_scanning: gl-sast-report.json


secret_detection:
  stage: test
  script:
    - echo "Running secret detection scan ..."

  artifacts:
    reports:
      container_scanning: gl-secret-detection-report.json



trivy_container_scanning:
  stage: test

  services:
    - name: $CI_REGISTRY/mouse-informatics/dind:latest
      alias: docker
  
  rules:
    - if: '$CI_COMMIT_REF_NAME == "master"'
      when: on_success
      allow_failure: true
  
  before_script:
    - export TRIVY_VERSION=$(wget -qO - "https://api.github.com/repos/aquasecurity/trivy/releases/latest" | grep '"tag_name":' | sed -E 's/.*"v([^"]+)".*/\1/')
    - echo $TRIVY_VERSION
    - wget --no-verbose https://github.com/aquasecurity/trivy/releases/download/v${TRIVY_VERSION}/trivy_${TRIVY_VERSION}_Linux-64bit.tar.gz -O - | tar -zxvf -
    - echo "${CI_REGISTRY_PASSWORD}" | docker login -u "${CI_REGISTRY_USER}" --password-stdin  ${CI_REGISTRY}
    
    - source ${MOUNT_POINT}/datetime.env
    - echo ${NOW}

  script:
    # Build report
    - ./trivy --cache-dir .trivycache/ image --exit-code 0 --no-progress --format template --template "@contrib/gitlab.tpl" -o gl-container-scanning-report.json "${CI_REGISTRY_IMAGE}":"${NOW}"
    # Print report
    - ./trivy --cache-dir .trivycache/ image --exit-code 0 --no-progress --severity HIGH "${CI_REGISTRY_IMAGE}":"${NOW}"
    # Fail on critical vulnerability
    - ./trivy --cache-dir .trivycache/ image --exit-code 1 --severity CRITICAL --no-progress "${CI_REGISTRY_IMAGE}":"${NOW}"
 
    - docker logout ${CI_REGISTRY}

  cache:
    paths:
      - .trivycache/

  artifacts:
    reports:
      container_scanning: gl-container-scanning-report.json




hh-dev-deploy:
  stage: dev-deploy
  # Use an image with helm v2.14.3, kubectl v1.15.2, alpine 3.10
  # rancher/hyperkube:v1.15.3-rancher1 installed on the hh cluster
  image: $CI_REGISTRY/mouse-informatics/helm-kubectl-alpine:latest
  only:
    refs:
      - master
  script:
    - source ${MOUNT_POINT}/datetime.env
    - echo ${NOW}
    #
    - kubectl config set-cluster local --server="${HH_KUBERNETES_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${HH_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
    - kubectl config set-credentials ${HH_KUBERNETES_DEV_USER} --token="${HH_KUBERNETES_DEV_USER_TOKEN}"
    - kubectl config set-context "${HH_KUBERNETES_DEV_NAMESPACE}" --cluster=local --user=${HH_KUBERNETES_DEV_USER} --namespace="${HH_KUBERNETES_DEV_NAMESPACE}"
    - kubectl config use-context "${HH_KUBERNETES_DEV_NAMESPACE}"
    - kubectl version
    #
    - sed -i "s/latest/${NOW}/g" kube/wp/dev/database/database-deployment.yaml
    - sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/dev/database/database-deployment.yaml
    
    - |
      if kubectl apply -f kube/wp/dev/database/database-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/dev/database/database-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi


    - kubectl rollout status -f kube/wp/dev/database/database-deployment.yaml
    - kubectl get pod,deployment,rs



hx-dev-deploy:
  stage: dev-deploy
  # Use an image with helm v2.14.3, kubectl v1.15.2, alpine 3.10
  # rancher/hyperkube:v1.15.3-rancher1 installed on the hh cluster
  image: $CI_REGISTRY/mouse-informatics/helm-kubectl-alpine:latest
  only:
    refs:
      - master
  script:
    - source ${MOUNT_POINT}/datetime.env
    - echo ${NOW}
    #
    - kubectl config set-cluster local --server="${HX_KUBERNETES_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${HX_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
    - kubectl config set-credentials ${HX_KUBERNETES_DEV_USER} --token="${HX_KUBERNETES_DEV_USER_TOKEN}"
    - kubectl config set-context "${HX_KUBERNETES_DEV_NAMESPACE}" --cluster=local --user=${HX_KUBERNETES_DEV_USER} --namespace="${HX_KUBERNETES_DEV_NAMESPACE}"
    - kubectl config use-context "${HX_KUBERNETES_DEV_NAMESPACE}"
    - kubectl version
    #
    - sed -i "s/latest/${NOW}/g" kube/wp/dev/database/database-deployment.yaml
    - sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/dev/database/database-deployment.yaml
    
    - |
      if kubectl apply -f kube/wp/dev/database/database-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/dev/database/database-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi


    - kubectl rollout status -f kube/wp/dev/database/database-deployment.yaml
    - kubectl get pod,deployment,rs




hh-dev-test:
    image: $CI_REGISTRY/mouse-informatics/ubuntu:latest
    stage: dev-test
    before_script:
        - apt-get update && apt-get install -y curl && apt-get clean && rm -rf /var/lib/apt/lists/*
    script:
        
        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"
        
        - source "$CI_PROJECT_DIR"/scripts/service_test.sh -s="${HH_KUBERNETES_INTERNAL_ENDPOINT}/${HH_KUBERNETES_DEV_NAMESPACE}/v1/graphql"  | tee ${MOUNT_POINT}/hh-dev-test.log
        

    artifacts:
        paths:
            - "$MOUNT_POINT/"




hx-dev-test:
    image: $CI_REGISTRY/mouse-informatics/ubuntu:latest
    stage: dev-test
    before_script:
        - apt-get update && apt-get install -y curl && apt-get clean && rm -rf /var/lib/apt/lists/*
    script:
        
        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"
        
        - source "$CI_PROJECT_DIR"/scripts/service_test.sh -s="${HX_KUBERNETES_INTERNAL_ENDPOINT}/${HX_KUBERNETES_DEV_NAMESPACE}/v1/graphql"  | tee ${MOUNT_POINT}/hx-dev-test.log
        

    artifacts:
        paths:
            - "$MOUNT_POINT/"



hh-production-deploy:
  stage: production-deploy
  # Use an image with helm v2.14.3, kubectl v1.15.2, alpine 3.10
  # rancher/hyperkube:v1.15.3-rancher1 installed on the hh cluster
  image: $CI_REGISTRY/mouse-informatics/helm-kubectl-alpine:latest
  only:
    refs:
      - master
  script:
    - source ${MOUNT_POINT}/datetime.env
    - echo ${NOW}
    #
    - kubectl config set-cluster local --server="${HH_KUBERNETES_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${HH_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
    - kubectl config set-credentials ${HH_KUBERNETES_USER} --token="${HH_KUBERNETES_USER_TOKEN}"
    - kubectl config set-context "${HH_KUBERNETES_NAMESPACE}" --cluster=local --user=${HH_KUBERNETES_USER} --namespace="${HH_KUBERNETES_NAMESPACE}"
    - kubectl config use-context "${HH_KUBERNETES_NAMESPACE}"
    - kubectl version
    #
    - sed -i "s/latest/${NOW}/g" kube/wp/production/database/database-deployment.yaml
    - sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/production/database/database-deployment.yaml
    
    - |
      if kubectl apply -f kube/wp/production/database/database-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/production/database/database-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi


    - kubectl rollout status -f kube/wp/production/database/database-deployment.yaml
    - kubectl get pod,deployment,rs



hx-production-deploy:
  stage: production-deploy
  # Use an image with helm v2.14.3, kubectl v1.15.2, alpine 3.10
  # rancher/hyperkube:v1.15.3-rancher1 installed on the hh cluster
  image: $CI_REGISTRY/mouse-informatics/helm-kubectl-alpine:latest
  only:
    refs:
      - master
  script:
    - source ${MOUNT_POINT}/datetime.env
    - echo ${NOW}
    #
    - kubectl config set-cluster local --server="${HX_KUBERNETES_ENDPOINT}"
    - kubectl config set clusters.local.certificate-authority-data "${HX_KUBERNETES_CERTIFICATE_AUTHORITY_DATA}"
    - kubectl config set-credentials ${HX_KUBERNETES_USER} --token="${HX_KUBERNETES_USER_TOKEN}"
    - kubectl config set-context "${HX_KUBERNETES_NAMESPACE}" --cluster=local --user=${HX_KUBERNETES_USER} --namespace="${HX_KUBERNETES_NAMESPACE}"
    - kubectl config use-context "${HX_KUBERNETES_NAMESPACE}"
    - kubectl version
    #
    - sed -i "s/latest/${NOW}/g" kube/wp/production/database/database-deployment.yaml
    - sed -i "s/STRING_REPLACED_DURING_REDEPLOY/$(date)/g" kube/wp/production/database/database-deployment.yaml
    
    - |
      if kubectl apply -f kube/wp/production/database/database-deployment.yaml --record | grep -q unchanged; then
          echo "=> Patching deployment to force image update."
          kubectl patch -f kube/wp/production/database/database-deployment.yaml --record -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"ci-last-updated\":\"$(date +'%s')\"}}}}}"
      else
          echo "=> Deployment apply has changed the object, no need to force image update."
      fi


    - kubectl rollout status -f kube/wp/production/database/database-deployment.yaml
    - kubectl get pod,deployment,rs





hh-production-test:
    image: $CI_REGISTRY/mouse-informatics/ubuntu:latest
    stage: production-test
    before_script:
        - apt-get update && apt-get install -y curl && apt-get clean && rm -rf /var/lib/apt/lists/*
    script:
        
        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"
        
        - source "$CI_PROJECT_DIR"/scripts/service_test.sh -s="${HH_KUBERNETES_INTERNAL_ENDPOINT}/${HH_KUBERNETES_NAMESPACE}/v1/graphql"  | tee ${MOUNT_POINT}/hh-production-test.log
        

    artifacts:
        paths:
            - "$MOUNT_POINT/"




hx-production-test:
    image: $CI_REGISTRY/mouse-informatics/ubuntu:latest
    stage: production-test
    before_script:
        - apt-get update && apt-get install -y curl && apt-get clean && rm -rf /var/lib/apt/lists/*
    script:
        
        # Establish a data directory
        - mkdir -p "$MOUNT_POINT"
        - cd "$MOUNT_POINT"
        
        - source "$CI_PROJECT_DIR"/scripts/service_test.sh -s="${HX_KUBERNETES_INTERNAL_ENDPOINT}/${HX_KUBERNETES_NAMESPACE}/v1/graphql"  | tee ${MOUNT_POINT}/hx-production-test.log
        

    artifacts:
        paths:
            - "$MOUNT_POINT/"

